# vitber_indmatprosjekt_transformermodell

12.03.2024

- Oskar Feed Jakobsen
- Thorbjørn Djupvik
- Vemund Aakre

---

## Hensikt
Målet med dette prosjektet er å forstå hvordan dyp læring (deep learning) fungerer.
Mer spesifikt skal vi implementere transformermodellen som er av hovedkomponentene
i store språkmodeller som ChatGPT.

Testproblemene vi skal studere er å sortere en liste med tall og addere to heltall.
Vi skal se hvordan vi kan formulere dette som et problem der målet er å predikere
neste heltall i en sekvens. Dette er selvsagt en utrolig tungvint måte å implementere
sortering og addisjon på. Poenget er å se hvordan ulike problemer kan struktureres som
prediksjoner av heltall og dette gir en god forståelse av språkmodeller siden de fungerer
ut i fra samme prinsipp. Prosjektet har også en frivillig bonusoppgave der vi modellerer
språk ved å predikere neste bokstav i en setning.
