{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## 1.1\n",
    "\n",
    "We want to train a model that predicts $d$ given $d = a \\cdot b + c$.\n",
    "$a$, $b$ and $c$ are non-negative and $a$ and $c$ are two-digit integers and $b$ is a one-digit integer.\n",
    "This makes $d$ at most a three digit number. Specifically $d \\in \\{ 0, 1, 2, \\dots, 989, 990 \\}$. The representation of $d$ then becomes\n",
    "$n_0 n_1 n_2$.\n",
    "Because we reverse the digits, the training set $\\{x, y\\}$ would become:\n",
    "\n",
    "\\begin{align}\n",
    "    x &= [ a_0, a_1, b, c_0, c_1, d_2, d_1 ] \\\\\n",
    "    y &= [ a_1, b, c_0, c_1, d_2, d_1, d_0 ]\n",
    "\\end{align}\n",
    "\n",
    "A concrete example shows that padding with zeros keeps the length constant:\n",
    "\n",
    "$$\n",
    "    a = 5, b = 5, c = 33 \\\\\n",
    "    a \\cdot b + c = 58\n",
    "$$\n",
    "\n",
    "gives\n",
    "\n",
    "\\begin{align*}\n",
    "    x &= [0,5,5,3,3,8,5] \\\\\n",
    "    y &= [5,5,3,3,8,5,0].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "\n",
    "When the model is optimized it will predict d given a, b and c. Using the same $a = 5, b = 5, c = 33$ as before:\n",
    "\n",
    "\\begin{align}\n",
    "    x^{(0)} &= [0, 5, 5, 3, 3],  &[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(1)}, \\textcolor{red}{\\hat{z}_4^{(0)}}] = f_\\theta(x^{(0)})\\\\\n",
    "    x^{(1)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}],  &[\\hat{z}_0^{(1)}, \\cdots, \\textcolor{blue}{\\hat{z}_5^{(1)}}] = f_\\theta(x^{(1)})  \\\\\n",
    "    x^{(2)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{green}{\\hat{z}_6^{(2)}}] = f_\\theta(x^{(2)}) \\\\\n",
    "    x^{(3)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{purple}{\\hat{z}_7^{(3)}}] = f_\\theta(x^{(3)}) \\\\\n",
    "    x^{(4)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}, \\textcolor{purple}{\\hat{z}_7^{(3)}}]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = [\\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}, \\textcolor{purple}{\\hat{z}_7^{(3)}}]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annen mulighet\n",
    "\n",
    "\\begin{align}\n",
    "    x^{(0)} &= [0, 5, 5, 3, 3],  &[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(1)}, \\textcolor{red}{0}] = f_\\theta(x^{(0)})\\\\\n",
    "    x^{(1)} &= [0, 5, 5, 3, 3, \\textcolor{red}{0}],  &[\\hat{z}_0^{(1)}, \\cdots, \\textcolor{blue}{5}] = f_\\theta(x^{(1)})  \\\\\n",
    "    x^{(2)} &= [0, 5, 5, 3, 3,  \\textcolor{red}{0}, \\textcolor{blue}{5}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{green}{8}] = f_\\theta(x^{(2)}) \\\\\n",
    "    x^{(3)} &= [0, 5, 5, 3, 3, \\textcolor{red}{0}, \\textcolor{blue}{5}, \\textcolor{green}{8}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = [\\textcolor{red}{0}, \\textcolor{blue}{5}, \\textcolor{green}{8}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "\n",
    "Using cross entropy as the object function, with $m = 5$ and $y = [4, 3, 2, 1]$. If the object function $\\mathcal{L(\\theta, D)} = 0$, then $\\hat{Y}$ would be the onehot representation of $y$: \n",
    "\\begin{align}\n",
    "\\hat{Y} = onehot(y) =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In this case $ \\hat{y} = y = [4, 3, 2, 1]$.\n",
    "If the objectfunction $\\mathcal{L(\\theta, D)} = 0$, then $ \\hat{y}$ will be the as the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "Given $ d, m, n_{max}, k, p, L$. To find the amount of paramters that must be determined one msut look at the dimensions of all the matrices involved in the transformer model.\n",
    "\n",
    "$W_{E},  W_{U} \\in \\mathbb{R}^{d \\times m}$ and $W_{P} \\in \\mathbb{R}^{d \\times n_{max}}$ is only made once per transformer model\n",
    "\n",
    "$W_{O},  W_{V}, W_{Q},  W_{K} \\in \\mathbb{R}^{k \\times d}$ and $W_{1},  W_{2} \\in \\mathbb{R}^{p \\times d}$ are made for all $L$ layers in the transformer model.\n",
    "\n",
    "In total that gives $ 2 \\cdot m n + dn_{max} + L(kd +pd)$ individual paramters that must be determined\n",
    " "
   ]
  },

    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5  \n",
    "For a transformer model with parameters $ W_{O}, W_{V}, W_{Q}, W_{K}, W_{1}, W_{2}, W_{U} = I_{2 \\times 2}$ and $\\sigma(x) = Relu(x)=max(0, x)$  and \n",
    "\\begin{align} W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} , W_{P} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align}\n",
    "  Given an input $x=[1]$, $n = n_{max} = 1$, $m = d = k = p = 2$ and $L = 1$, alpha must be larger than 1 to get $\\hat{z} = [1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&X = onehot(x) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\\n",
    "&z_{0} = W_{E}X + [W_{P}]_{0:n} = \\begin{bmatrix}1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\n",
    "\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ \\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "&z_{1/2} = f_{d}^{A}(z_0) = z_0 + W_{O}^{T}W_{V}z_{0}A(z_0), W_{O}^{T}W_{V} = I_{2 \\times 2}I_{2 \\times 2} = 1 \\\\\n",
    "&= z_0 + z_0 softmax_{col}(z^{T}W_{Q}^{T}W_{K}z + D), D = [0], W_{Q}^{T}W_{K} = I_{2 \\times 2}I_{2 \\times 2} = 1\\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} softmax_{col}(1 + \\alpha^2) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "&z_1 = f_1^{F}(z_{1/2}) = z_{1/2} + W_2^{T}\\sigma(W_1z_{1/2}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}) \\\\ \n",
    "& = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix},\\ given \\ \\alpha > 0 \\\\\n",
    "\n",
    "&Z = softmax(\\begin{bmatrix}4 \\\\ \\alpha\\end{bmatrix}) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{4\\alpha}} \\\\ \\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} \\end{bmatrix} \\\\\n",
    "&\\hat{z} = argmax_{col}(z) \\\\\n",
    "\n",
    "&\\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} > \\frac{e^{4}}{e^4+e^{4\\alpha}}, \\ if \\ \\hat{z} = [1] \\\\\n",
    "&e^{4\\alpha} > e^{4} \\\\\n",
    "&4\\alpha > 4 \\\\\n",
    "&\\alpha > 1  \\ \\blacksquare\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works given $\\alpha > 0$. For $\\alpha \\leq 0$ one must look from eq. (6) where $\\sigma = max(0, x)$\n",
    "\n",
    "\\begin{align}\n",
    "&z_1 = f_1^{F}(z_{1/2}) = z_{1/2} + W_2^{T}\\sigma(W_1z_{1/2}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}) \\\\\n",
    "&= \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix},\\ given \\ \\alpha < 0 \\\\\n",
    "\n",
    "&Z = softmax(\\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix}) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} \\end{bmatrix} \\\\\n",
    "&\\hat{z} = argmax_{col}(\\begin{bmatrix} \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} \\end{bmatrix}) \\\\\n",
    "&\\frac{e^2\\alpha}{e^4+e^{2\\alpha}} > \\frac{e^4}{e^4+e^{2\\alpha}}, \\ if \\ \\hat{z} = [1] \\\\ \n",
    "&\\nexists \\ \\alpha \\leq 0 \\ s.t. \\ e^{2\\alpha} > e^{4}\\\\\n",
    "&\\rightarrow \\alpha > 0\n",
    "\n",
    "\\end{align}\n",
    "and therefore $\\alpha > 1$ as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1\n",
    "\n",
    "To perfrom a step of gradient descent of a NeuralNetwok, the member function \"step_gd\" is called. This function then calls on the function \"step_gd\" in the class Layer. Layer is the base class for alot of the other classes used in this project, for example Attention, SoftMax and LinearLayer. These child classes will then inherit the member function \"step_gd\" from their parent class, Layer. The objects initialzed from these classes make up the neural network and then inherit the function \"step_gd\". "
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep all the parameters (dimensions, length of input/output, number of iterations etc.) we decided to store these in separate dataclasses in `train_test_params.py`. See the comments in `BaseParams` for an explanation of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_params import *\n",
    "\n",
    "sort_params1 = SortParams1()    # parameters in part 1 of exercise 3.3\n",
    "sort_params2 = SortParams2()    # parameters in part 2 of exercise 3.3\n",
    "add_params = AddParams()\n",
    "text_params = TextGenParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "TODO: use `test_implementation.ipynb` or `test_implementation.py`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "The file `train_network.py` has two functions: `init_neural_network` and `train_network`.\n",
    "\n",
    "TODO: write stuff idk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Sorting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training and testing data for the sorting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params1.r,\n",
    "    num_ints=sort_params1.m,\n",
    "    samples_per_batch=sort_params1.D,\n",
    "    n_batches_train=sort_params1.b_train,\n",
    "    n_batches_test=sort_params1.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params1.r - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(sort_params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using `CrossEntropy` as the loss function (object function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params1.alpha,\n",
    "    n_iter=sort_params1.n_iter,\n",
    "    num_ints=sort_params1.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(L)\n",
    "ax.set(title=\"Object function\", ylabel=r\"$\\mathcal{L}$\", xlabel=\"iteration step\", yscale=\"log\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load a pre-trained network from a pickle dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open(\"nn_dump_exer3.pkl\", \"rb\") as f:\n",
    "    network = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the object function has converged to a minima. This is well supported by testing the network on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=sort_params1.m\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the model to sort sequences where $r = 7$ and $m = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params2.r,\n",
    "    num_ints=sort_params2.m,\n",
    "    samples_per_batch=sort_params2.D,\n",
    "    n_batches_train=sort_params2.b_train,\n",
    "    n_batches_test=sort_params2.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params2.r - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(sort_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train[:3],\n",
    "    y_train=y_train[:3],\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params2.alpha/10,\n",
    "    n_iter=200,\n",
    "    num_ints=sort_params2.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")\n",
    "L2 = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train[3:6],\n",
    "    y_train=y_train[3:6],\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params2.alpha/10,\n",
    "    n_iter=200,\n",
    "    num_ints=sort_params2.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")\n",
    "L3 = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train[6:],\n",
    "    y_train=y_train[6:],\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params2.alpha/10,\n",
    "    n_iter=200,\n",
    "    num_ints=sort_params2.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.concatenate((L, L2, L3)))\n",
    "ax.set(title=\"Object function\", ylabel=r\"$\\mathcal{L}$\", xlabel=\"iteration step\", yscale=\"log\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=sort_params2.m\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(add_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "\n",
    "# prepare training and test data for addition problem\n",
    "training_data = get_train_test_addition(\n",
    "    n_digit = add_params.r,\n",
    "    samples_per_batch = add_params.D,\n",
    "    n_batches_train = add_params.b_train,\n",
    "    n_batches_test=add_params.b_test\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, add_params.r*2 - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"][:, :, ::-1]    # remember that (c0, c1, c2) is reversed in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=add_params.alpha,\n",
    "    n_iter=add_params.n_iter,\n",
    "    num_ints=add_params.m,\n",
    "    dump_to_pickle_file=True,\n",
    "    file_name_dump=\"nn_addition.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open(\"nn_dump_add.pkl\", \"rb\") as f:\n",
    "    network = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=add_params.m\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitber_indmatprosjekt_transformermodell-j3GqZX7I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
