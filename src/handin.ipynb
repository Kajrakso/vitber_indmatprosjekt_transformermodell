{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Transformer model to predict sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer model is a form of deep learning architecture. It is used to predict what should come next given an input sequence. As all other deep learning models, a transformer must also be trained. An attention layer makes sure that information is propagated from earlier in the sequence to the later parts of the sequence. In this way, it is easier for the model to connect all the data from the input sequence and give a better prediction. In this notebook neural networks will be trained to sort a sequence of integers and add together two two-digit non-negative integer numbers. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Introductory exercises\n",
    "\n",
    "### 1.1\n",
    "\n",
    "We want to train a model that predicts $d$ given $d = a \\cdot b + c$.\n",
    "$a$, $b$ and $c$ are non-negative integers. $a$ and $c$ are two-digit integers, while $b$ is a one-digit integer.\n",
    "This makes $d$ at most a three digit number. Specifically $d \\in \\{ 0, 1, 2, \\dots, 989, 990 \\}$. The representation of $d$ then becomes\n",
    "$n_0 n_1 n_2$.\n",
    "Because we reverse the digits, the training set $\\{x, y\\}$ would become:\n",
    "\n",
    "\\begin{align*}\n",
    "    x &= [ a_0, a_1, b, c_0, c_1, d_2, d_1 ] \\\\\n",
    "    y &= [  d_2, d_1, d_0 ]\n",
    "\\end{align*}\n",
    "\n",
    "A concrete example shows that padding $a$ with a leading zeros keeps the length constant:\n",
    "\n",
    "$$\n",
    "    a = 5, b = 5, c = 33 \\\\\n",
    "    a \\cdot b + c = 58\n",
    "$$\n",
    "\n",
    "gives\n",
    "\n",
    "\\begin{align*}\n",
    "    x &= [0,5,5,3,3,8,5] \\\\\n",
    "    y &= [8,5,0].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "\n",
    "When the network is optimized it will predict $d$ given $a$, $b$ and $c$. Using the same $a = 5, b = 5, c = 33$ as before:\n",
    "\n",
    "\\begin{align*}\n",
    "    x^{(0)} &= [0, 5, 5, 3, 3],  &[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(0)}, \\textcolor{red}{8}] = f_\\theta(x^{(0)})\\\\\n",
    "    x^{(1)} &= [0, 5, 5, 3, 3, \\textcolor{red}{8}],  &[\\hat{z}_0^{(1)}, \\cdots, \\textcolor{blue}{5}] = f_\\theta(x^{(1)})  \\\\\n",
    "    x^{(2)} &= [0, 5, 5, 3, 3,  \\textcolor{red}{8}, \\textcolor{blue}{5}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{green}{0}] = f_\\theta(x^{(2)}) \\\\\n",
    "    x^{(3)} &= [0, 5, 5, 3, 3, \\textcolor{red}{8}, \\textcolor{blue}{5}, \\textcolor{green}{0}] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} = [\\textcolor{red}{8}, \\textcolor{blue}{5}, \\textcolor{green}{0}]\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "Using cross entropy as the object function, with $m = 5$ and $y = [4, 3, 2, 1]$. If the object function $\\mathcal{L(\\theta, D)} = 0$, then $\\hat{Y}$ would be the onehot representation of the correct solution $y$: \n",
    "\\begin{align*}\n",
    "\\hat{Y} = \\text{onehot}(y) =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In this case $ \\hat{y} = y = [4, 3, 2, 1]$.\n",
    "If the object function $\\mathcal{L(\\theta, D)} = 0$, then $ \\hat{y}$ will be the same as the solution $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "Given $ d, m, n_{max}, k, p, L$. To find the amount of parameters that must be determined one must look at the dimensions of all the matrices involved in the neural network.\n",
    "\n",
    "$W_{E},  W_{U} \\in \\mathbb{R}^{d \\times m}$ and $W_{P} \\in \\mathbb{R}^{d \\times n_{max}}$ are only made once per neural network.\n",
    "\n",
    "$W_{O},  W_{V}, W_{Q},  W_{K} \\in \\mathbb{R}^{k \\times d}$ and $W_{1},  W_{2} \\in \\mathbb{R}^{p \\times d}$ are made for all $L$ layers in the transformer.\n",
    "\n",
    "In total that gives $ 2 \\cdot dm  + dn_{max} + L(4\\cdot kd + 2\\cdot pd)$ individual parameters that must be determined.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  \n",
    "For a neural network with parameters $ W_{O}, W_{V}, W_{Q}, W_{K}, W_{1}, W_{2}, W_{U} = I_{2 \\times 2}$,  $\\sigma(x) = \\text{Relu}(x)=\\text{max}(0, x)$  and \n",
    "\\begin{align*} W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} \\ \\text{and} \\ W_{P} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align*}\n",
    "  Given an input $x=[1]$, $n = n_{max} = 1$, $m = d = k = p = 2$ and $L = 1$, $\\alpha$ must be larger than 1 to get $\\hat{z} = [1]$ as a prediction, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "X &= \\text{onehot}(x) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\\n",
    "z_{0} &= W_{E}X + [W_{P}]_{0:n} = \\begin{bmatrix}1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\n",
    "\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ \\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "z_{\\frac{1}{2}} &= f_{d}^{A}(z_0) = z_0 + W_{O}^{T}W_{V}z_{0}A(z_0), &\\ W_{O}^{T}W_{V} = I_{2 \\times 2}I_{2 \\times 2} = 1 \\\\\n",
    "&= z_0 + z_0 \\text{softmax}_{\\text{col}}(z^{T}W_{Q}^{T}W_{K}z + D), &\\ D = [0], \\ W_{Q}^{T}W_{K} = I_{2 \\times 2}I_{2 \\times 2} = 1\\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} \\text{softmax}_{\\text{col}}(1 + \\alpha^2) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "z_1 &= f_1^{F}(z_{\\frac{1}{2}}) = z_{\\frac{1}{2}} + W_2^{T}\\sigma(W_1z_{\\frac{1}{2}}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma\\left(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}\\right) \\\\ \n",
    "&= \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix},&\\ \\text{given} \\ \\alpha > 0 \\\\\n",
    "\n",
    "Z &= \\text{softmax}_{\\text{col}}\\left(\\begin{bmatrix}4 \\\\ \\alpha\\end{bmatrix}\\right) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{4\\alpha}} \\\\ \\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} \\end{bmatrix} \\\\\n",
    "\\hat{z} &= \\text{argmax}_{\\text{col}}(Z) \\\\\n",
    "\\end{align}\n",
    " \n",
    "$\\text{if} \\ \\hat{z} = [1] \\ \\text{then we must have that}$ \n",
    "\\begin{align*} \n",
    "\\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} &> \\frac{e^{4}}{e^4+e^{4\\alpha}}\\\\\n",
    "e^{4\\alpha} &> e^{4} \\\\\n",
    "4\\alpha &> 4 \\\\\n",
    "\\alpha &> 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works given $\\alpha > 0$. For $\\alpha \\leq 0$ one must look from eq. (6) where $\\sigma = \\text{max}(0, x)$\n",
    "\n",
    "\\begin{align*}\n",
    "z_1 &= f_1^{F}(z_{\\frac{1}{2}}) = z_{\\frac{1}{2}} + W_2^{T}\\sigma(W_1z_{\\frac{1}{2}}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma\\left(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}\\right) \\\\\n",
    "&= \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix},&\\ \\text{given} \\ \\alpha \\leq 0 \\\\\n",
    "\n",
    "Z &= \\text{softmax}_{\\text{col}}\\left(\\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix}\\right) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} \\end{bmatrix} \\\\\n",
    "\\hat{z} &= \\text{argmax}_{\\text{col}}\\left(Z\\right)\n",
    "\\end{align*}\n",
    "\n",
    "If $\\hat{z} = [1]$ we must have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} &> \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \n",
    "\\end{align*}\n",
    "But $\\nexists \\alpha \\leq 0 \\ \\text{s.t.} \\ e^{2\\alpha} > e^{4} $ and therefore $\\alpha > 1$. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Implementing the network\n",
    "\n",
    "### 2.1\n",
    "\n",
    "To perform a gradient descent step on a `NeuralNetwork`, the member function `step_gd` is called. This function then calls on the `step_gd` method for all the relevant layers in the network. `step_gd` has a default implementation in the class `Layer`, and all subclasses of `Layer`  will inherit this method unless specifically overwritten in the given subclass. All of the layers used in the network are subclasses of `Layer` and thus inherit `step_gd` from it, although layers such as `FeedForward` and `EmbedPosition` overwrite it with a custom implementation of their own. Even though these classes overwrite the inherited method, they can still use it by calling `super().step_gd()` in their own implementation. This is for example done in `EmbedPosition`. `FeedForward` on the other hand uses the `step_gd` of its two sublayers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 and 2.3\n",
    "\n",
    "We refer to the files `layers.py` and `layers_numba.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras\n",
    "\n",
    "#### `Numba`\n",
    "\n",
    "In an effort to speed up our code, we reimplemented all the layers using numba. By doing so we managed to get the most resource intensive part of the training compiled to much faster machine code. The downside is that it spends a lot of time compiling the code before it runs. For long running training sessions, this is worth it because of the speed up. However, for programs running for only about a minute or less, the compilation overhead is too big to save any time, and the normal implementation finishes first. Another note to make is that when running JIT-compiled code, we get some performance warnings from the compiler. These originate from a part of the code that tries to do operations on the columns of three dimensional matrices, thus producing several cache misses. This problem can be solved in different ways, but we found that the current implementation worked the best nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dumping and loading\n",
    "\n",
    "When starting to train bigger models, we found that a way to save and load the trained networks would be beneficial. Storing the original layers, non-numba layers, proved to be really simple with `pickle` or `dill`. The numba-layers on the other hand could not be serialized as is, and for that reason we had to implement some extra utility functions for converting the data into a serializable format to be dumped with `dill`. These functions were implemented in `neural_network.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Training the nework and solving problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep all the parameters (dimensions, length of input/output, number of iterations etc.) at one place we decided to store these in separate dataclasses in `train_test_params.py`. See the comments in `BaseParams` for an explanation of the member variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_params import SortParams1, SortParams2, AddParams\n",
    "\n",
    "sort_params1 = SortParams1()    # parameters in part 1 of exercise 3.3\n",
    "sort_params2 = SortParams2()    # parameters in part 2 of exercise 3.3\n",
    "add_params = AddParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "\n",
    "`test_implementation.ipynb` was used to test the implementation of the layers. We had to make a few changes to the tests:\n",
    "\n",
    "When computing the value of our loss function we have to slice $Z$ such that we only use the columns that are useful.\n",
    "\n",
    "How to slice is determined by making sure that all $x$'s and $y$'s follow the dimensions given in part 3.2.1 in the project description.\n",
    "\n",
    "```python\n",
    "L = loss.forward(Z[:, :, -n_y:], y)\n",
    "```\n",
    "In addition we have to take into account that the backward pass of Softmax is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z} := g_l \\odot z_l - \\text{sum}_\\text{col}(g_l \\odot S) \\odot P.\n",
    "\\end{align*}\n",
    "\n",
    "Here $g_l \\in \\R^{m \\times n_y}$ and $z_l \\in \\R ^{m \\times n}$. In order to match the dimensions we left-pad the gradient with zeros:\n",
    "\n",
    "```python\n",
    "pad_matrix = np.zeros((b, m, n_max - n_y))\n",
    "grad_Z = np.concatenate((pad_matrix, grad_Z), axis=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "\n",
    "The file `train_network.py` has two functions:\n",
    "\n",
    "- `init_neural_network` creates an object of type `NeuralNetwork` given the parameters that the neural network will use.\n",
    "\n",
    "- `train_network` is the implementation of Algorithm 4 in the assignment. It trains the neural network and uses the Adam algorithm to change the individual parameters of the network for each iteration. We also added support for dumping the network during training.\n",
    "\n",
    "In order to look at how the loss function develops we plot the result on a logarithmically scaled y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss(loss: list[np.ndarray], title: str, labels: None|tuple[str] = None) -> None:\n",
    "    \"\"\" Plots the loss as a function of iteration step\n",
    "\n",
    "    Args:\n",
    "        loss: list of loss values\n",
    "        title: title of the plot\n",
    "        labels: optional labels\n",
    "    \"\"\"\n",
    "    iters = [];L = []\n",
    "    for l in loss:\n",
    "        iters.append([i+1 for i in range(len(l[l > 0]))])\n",
    "        L.append(l[l>0])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    if labels == None:\n",
    "        for i, l in zip(iters, L):\n",
    "            ax.plot(i, l)\n",
    "    else:\n",
    "        for i, l, lab in zip(iters, L, labels):\n",
    "            ax.plot(i, l, label=lab)\n",
    "    ax.set(\n",
    "        title=title,\n",
    "        ylabel=r\"Loss $\\mathcal{L}$\",\n",
    "        xlabel=\"Iterations\",\n",
    "        yscale=\"log\",\n",
    "    )\n",
    "    ax.grid()\n",
    "    if labels != None: ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Sorting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple sorting of binary sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sort sequences only consisting of zeros and ones. A quick example never hurts:\n",
    "\n",
    "$$\n",
    "[0,1,1,1,0] \\to [0,0,1,1,1]\n",
    "$$\n",
    "\n",
    "Prepare training and testing data for the sorting problem described in the first part of exercise 3.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params1.r,\n",
    "    num_ints=sort_params1.m,\n",
    "    samples_per_batch=sort_params1.D,\n",
    "    n_batches_train=sort_params1.b_train,\n",
    "    n_batches_test=sort_params1.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params1.r - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the network with r = $ 5$ and m = $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(sort_params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using `CrossEntropy` as the loss function (object function).\n",
    "\n",
    "If the value of our object function $\\mathcal{L}$ gets lower than $0.01$ we stop the training.\n",
    "Otherwise the training runs for $n_{\\text{iter}} = 300$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params1.alpha,\n",
    "    n_iter=sort_params1.n_iter,\n",
    "    num_ints=sort_params1.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss([L], \"Training for the sorting problem part 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load a pre-trained network from a pickle dump as explained in part 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_network = init_neural_network(sort_params1)\n",
    "loaded_network.load(\"SUPPLY_FILENAME_HERE\")            # this will override all parameters initialized by init_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: It seems that the object function has converged to a minima. This is well supported by testing the network on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=sort_params1.m\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add comment on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling up the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously set the sequence length $r = 5$ and vocabulary size $m = 2$. The total amount of different sequences that can be generated is $2^5=32$. Since $2500$ sequences are created to train the neural network, there will most likely be extremely few or no new sequences to test the model on. Therefore the neural network will most likely be very accurate because it has been trained on every possible input it can receive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the network to sort sequences where $r = 7$ and $m = 5$. This will result in $5^7 = 78125$ possible input sequences and thus we expect the testing data to contain unknown sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params2.r,\n",
    "    num_ints=sort_params2.m,\n",
    "    samples_per_batch=sort_params2.D,\n",
    "    n_batches_train=sort_params2.b_train,\n",
    "    n_batches_test=sort_params2.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params2.r - 1:]       # we slice y_train in order to get the correct size\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network, init_neural_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "network1 = init_neural_network(sort_params2)\n",
    "network2 = init_neural_network(sort_params2)\n",
    "network3 = init_neural_network(sort_params2)\n",
    "\n",
    "networks = (network1, network2, network3)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L1, L2, L3 = [\n",
    "    train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params2.alpha,\n",
    "    n_iter=sort_params2.n_iter,\n",
    "    num_ints=sort_params2.m,\n",
    "    dump_to_pickle_file=False,\n",
    ") for network in networks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss([L1, L2, L3], \"Training for the sorting problem part 2\", labels=(\"L1\", \"L2\", \"L3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "for n in networks:\n",
    "    test_trained_network(\n",
    "        network=n, x_test=x_test, y_test=y_test, num_ints=sort_params2.m\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: https://datascience.stackexchange.com/questions/25024/strange-behavior-with-adam-optimizer-when-training-for-too-long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Addition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of integer addition.\n",
    "Given two two-digit non-negative integers we want to train our model to predict the sum going through the same procedure as for the problem of sorting integers.\n",
    "\n",
    "Note that we now have $10^4$ possible input sequences: $a_0, a_1, b_0, b_1 \\in \\{0,1,2,\\dots,9\\}$.\n",
    "The total number of training sequences are $ \\text{num\\_batches} \\cdot \\text{batch\\_size} = 20 \\cdot 250 = 5000$.\n",
    "Therefore the training set will not contain all possibilities and our network will encounter unknown data during the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "\n",
    "training_data = get_train_test_addition(\n",
    "    n_digit = add_params.r,\n",
    "    samples_per_batch = add_params.D,\n",
    "    n_batches_train = add_params.b_train,\n",
    "    n_batches_test=add_params.b_test\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, add_params.r*2 - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"][:, :, ::-1]    # remember that (c0, c1, c2) is reversed in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network, init_neural_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "network1 = init_neural_network(add_params)\n",
    "network2 = init_neural_network(add_params)\n",
    "network3 = init_neural_network(add_params)\n",
    "\n",
    "networks = (network1, network2, network3)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L1, L2, L3 = [\n",
    "    train_network(\n",
    "        network=network,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        loss_func=loss,\n",
    "        alpha=add_params.alpha,\n",
    "        n_iter=add_params.n_iter,\n",
    "        num_ints=add_params.m,\n",
    "        dump_to_pickle_file=False\n",
    "    ) for network in networks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss([L1, L2, L3], \"Training for the addition problem\", (\"net1\", \"net2\", \"net3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "\n",
    "for n in networks:\n",
    "    test_trained_network(\n",
    "        network=n, x_test=x_test, y_test=y_test, num_ints=add_params.m\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The transformer model and Adam combine to create a neural network that predicts sequences of symbols.\n",
    "\n",
    "We first saw rapid convergence of the loss function for the sorting problem with only zeros and ones.\n",
    "The network then sorted correctly $100 \\%$ of the test cases.\n",
    "Trying to sort longer sequences of integers ranging from $0$ to $4$ also resultet in near $100\\%$ score.\n",
    "\n",
    "When adding integers the training time increased.\n",
    "This is where `Numba` excels.\n",
    "After compilation the training process is considerably faster than what we managed to do without `Numba`.\n",
    "\n",
    "This is to be expected as the problem of addition is more involved than sorting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task: Text generation\n",
    "\n",
    "The cells below are copied from the provided `text_generation`-notebook with a few additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_params import TextGenParams\n",
    "\n",
    "text_params = TextGenParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import text_to_training_data\n",
    "\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    data,idx_to_text,text_to_idx, m = text_to_training_data(text_params.n_max,text,num_batches=text_params.b_train,batch_size=text_params.D)\n",
    "\n",
    "    print(\"We will train on %d batches of size %d\" % (len(data['x_train']),len(data['x_train'][0])))\n",
    "    print(\"Each sequence has length %d\" % text_params.n_max)\n",
    "\n",
    "    print(\"Example of a sequence (chars): \\n\")\n",
    "    print(''.join([idx_to_text[i] for i in data['x_train'][0][0]]))\n",
    "\n",
    "    print(\"\\nExample of a sequence (idx): \\n\")\n",
    "    print(data['x_train'][0][0])\n",
    "\n",
    "    text_params.m = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "net = init_neural_network(text_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network turns out to give (somewhat) decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "train_network.train_network(\n",
    "    network=net,\n",
    "    x_train=np.array(data[\"x_train\"]),\n",
    "    y_train=np.array(data[\"y_train\"]),\n",
    "    loss_func=loss,\n",
    "    alpha=text_params.alpha,\n",
    "    n_iter=text_params.n_iter,\n",
    "    num_ints=text_params.m,\n",
    "    dump_to_pickle_file=True,\n",
    "    file_name_dump=\"nn_text_gen.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "\n",
    "#We can now generate text from an initial string\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "#length of the total text sequence we want to generate\n",
    "n_gen = 10*text_params.n_max\n",
    "\n",
    "generated_idx = generate(net,start_idx,m,text_params.n_max,n_gen)\n",
    "\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do better.\n",
    "\n",
    "First of all we gathered more training data. From [Project Gutenberg](https://www.gutenberg.org/) we found the complete works of Shakespeare.\n",
    "It is found in the text file `spear.txt`.\n",
    "\n",
    "Then we scaled up the training process. We first increased $L$ from $2$ to $3$.\n",
    "After setting $\\mathcal{D} = 200$ datapoints per batch and using $100$ batches the model was trained for approximately $15$ hours on the computation server Markov at IMF.\n",
    "Only around $600$ iterations were done at this stage so we decreased the datapoints and batch size parameters. Then the model trained for approximately $9$ hours.\n",
    "The dumped networks and the corresponding loss functions are found in `markov_dumps/`. Let's now generate some text with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import text_to_training_data\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    data,idx_to_text,text_to_idx, m = text_to_training_data(text_params.n_max,text,num_batches=text_params.b_train,batch_size=text_params.D)\n",
    "\n",
    "    print(\"We will train on %d batches of size %d\" % (len(data['x_train']),len(data['x_train'][0])))\n",
    "    print(\"Each sequence has length %d\" % text_params.n_max)\n",
    "\n",
    "    print(\"Example of a sequence (chars): \\n\")\n",
    "    print(''.join([idx_to_text[i] for i in data['x_train'][0][0]]))\n",
    "\n",
    "    print(\"\\nExample of a sequence (idx): \\n\")\n",
    "    print(data['x_train'][0][0])\n",
    "\n",
    "    text_params.m = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "from train_network import init_neural_network\n",
    "\n",
    "markov_dump_dir = \"markov_dumps/\"\n",
    "\n",
    "crazy_net1 = init_neural_network(text_params)\n",
    "crazy_net1.load(markov_dump_dir + \"nn_dump_text_gen_markov.pkl\")\n",
    "\n",
    "print(\"----------\")\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "n_gen = 10*text_params.n_max\n",
    "generated_idx = generate(crazy_net1,start_idx,m,text_params.n_max,n_gen)\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)\n",
    "\n",
    "print(\"\\n----------\")\n",
    "start_text = \"Nay, but speak not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "n_gen = 10*text_params.n_max\n",
    "generated_idx = generate(crazy_net1,start_idx,m,text_params.n_max,n_gen)\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting how the loss function evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = np.load(\"markov_dumps/L_nn_dump_text_gen_markov_1.pkl\", allow_pickle=True)\n",
    "L2 = np.load(\"markov_dumps/L_nn_dump_text_gen_markov_2.pkl\", allow_pickle=True)\n",
    "plot_loss([ np.concatenate( (L1[L1>0], L2[L2 > 0]) ) ], \"Loss function after training on Markov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network only trained for X iterations.\n",
    "`FILENAM2` was trained for > iterations on SEERVERRR.\n",
    "Let's plot the loss function and generate some text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.load(\"oracle_dumps/L_nn_dump_text_gen_markov_1.pkl\", allow_pickle=True)\n",
    "plot_loss([ L[L > 0] ], \"Loss function after training on Oracle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "\n",
    "text_params.m = 100   # 100 unique symbols in speare.txt\n",
    "crazy_net2 = init_neural_network(text_params)\n",
    "crazy_net2.load(\"FILENAME2\")\n",
    "\n",
    "print(\"----------\")\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "n_gen = 10*text_params.n_max\n",
    "generated_idx = generate(crazy_net2,start_idx,m,text_params.n_max,n_gen)\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison here we generate text *without* training the network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "from train_network import init_neural_network\n",
    "\n",
    "text_params.m = 65   # 65 unique symbols in input.txt\n",
    "no_train_net = init_neural_network(text_params)\n",
    "\n",
    "#We can now generate text from an initial string\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "#length of the total text sequence we want to generate\n",
    "n_gen = 10*text_params.n_max\n",
    "\n",
    "generated_idx = generate(no_train_net,start_idx,m,text_params.n_max,n_gen)\n",
    "\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And then the animator suffered a fatal heart attack \n",
    "\n",
    "[Monty Python](https://www.youtube.com/watch?v=3Q2WPneqhhs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitber_indmatprosjekt_transformermodell-j3GqZX7I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
