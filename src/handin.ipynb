{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overskrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innleiing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## 1.1\n",
    "\n",
    "We want to train a model that predicts $d$ given $d = a \\cdot b + c$.\n",
    "$a$, $b$ and $c$ are non-negative and $a$ and $c$ are two-digit integers and $b$ is a one-digit integer.\n",
    "This makes $d$ at most a three digit number. Specifically $d \\in \\{ 0, 1, 2, \\dots, 989, 990 \\}$. The representation of $d$ then becomes\n",
    "$n_0 n_1 n_2$.\n",
    "Because we reverse the digits, the training set $\\{x, y\\}$ would become:\n",
    "\n",
    "\\begin{align}\n",
    "    x &= [ a_0, a_1, b, c_0, c_1, d_2, d_1 ] \\\\\n",
    "    y &= [ a_1, b, c_0, c_1, d_2, d_1, d_0 ]\n",
    "\\end{align}\n",
    "\n",
    "A concrete example shows that padding with zeros keeps the length constant:\n",
    "\n",
    "$$\n",
    "    a = 5, b = 5, c = 33 \\\\\n",
    "    a \\cdot b + c = 58\n",
    "$$\n",
    "\n",
    "gives\n",
    "\n",
    "\\begin{align*}\n",
    "    x &= [0,5,5,3,3,8,5] \\\\\n",
    "    y &= [5,5,3,3,8,5,0].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "\n",
    "When the network is optimized it will predict d given a, b and c. Using the same $a = 5, b = 5, c = 33$ as before:\n",
    "\n",
    "\\begin{align}\n",
    "    x^{(0)} &= [0, 5, 5, 3, 3],  &[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(1)}, \\textcolor{red}{\\hat{z}_4^{(0)}}] = f_\\theta(x^{(0)})\\\\\n",
    "    x^{(1)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}],  &[\\hat{z}_0^{(1)}, \\cdots, \\textcolor{blue}{\\hat{z}_5^{(1)}}] = f_\\theta(x^{(1)})  \\\\\n",
    "    x^{(2)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{green}{\\hat{z}_6^{(2)}}] = f_\\theta(x^{(2)}) \\\\\n",
    "    x^{(3)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{purple}{\\hat{z}_7^{(3)}}] = f_\\theta(x^{(3)}) \\\\\n",
    "    x^{(4)} &= [0, 5, 5, 3, 3, \\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}, \\textcolor{purple}{\\hat{z}_7^{(3)}}]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = [\\textcolor{red}{\\hat{z}_4^{(0)}}, \\textcolor{blue}{\\hat{z}_5^{(1)}}, \\textcolor{green}{\\hat{z}_6^{(2)}}, \\textcolor{purple}{\\hat{z}_7^{(3)}}]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annen mulighet\n",
    "\n",
    "\\begin{align}\n",
    "    x^{(0)} &= [0, 5, 5, 3, 3],  &[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(1)}, \\textcolor{red}{0}] = f_\\theta(x^{(0)})\\\\\n",
    "    x^{(1)} &= [0, 5, 5, 3, 3, \\textcolor{red}{0}],  &[\\hat{z}_0^{(1)}, \\cdots, \\textcolor{blue}{5}] = f_\\theta(x^{(1)})  \\\\\n",
    "    x^{(2)} &= [0, 5, 5, 3, 3,  \\textcolor{red}{0}, \\textcolor{blue}{5}],  &[\\hat{z}_0^{(2)}, \\cdots, \\textcolor{green}{8}] = f_\\theta(x^{(2)}) \\\\\n",
    "    x^{(3)} &= [0, 5, 5, 3, 3, \\textcolor{red}{0}, \\textcolor{blue}{5}, \\textcolor{green}{8}] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = [\\textcolor{red}{0}, \\textcolor{blue}{5}, \\textcolor{green}{8}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "\n",
    "Using cross entropy as the object function, with $m = 5$ and $y = [4, 3, 2, 1]$. If the object function $\\mathcal{L(\\theta, D)} = 0$, then $\\hat{Y}$ would be the onehot representation of $y$: \n",
    "\\begin{align}\n",
    "\\hat{Y} = \\text{onehot}(y) =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In this case $ \\hat{y} = y = [4, 3, 2, 1]$.\n",
    "If the objectfunction $\\mathcal{L(\\theta, D)} = 0$, then $ \\hat{y}$ will be the same as the solution $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "Given $ d, m, n_{max}, k, p, L$. To find the amount of parameters that must be determined one must look at the dimensions of all the matrices involved in the neural network.\n",
    "\n",
    "$W_{E},  W_{U} \\in \\mathbb{R}^{d \\times m}$ and $W_{P} \\in \\mathbb{R}^{d \\times n_{max}}$ is only made once per neural network.\n",
    "\n",
    "$W_{O},  W_{V}, W_{Q},  W_{K} \\in \\mathbb{R}^{k \\times d}$ and $W_{1},  W_{2} \\in \\mathbb{R}^{p \\times d}$ are made for all $L$ layers in the transformer model.\n",
    "\n",
    "In total that gives $ 2 \\cdot m n + dn_{max} + L(kd +pd)$ individual parameters that must be determined.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5  \n",
    "For a neural network with parameters $ W_{O}, W_{V}, W_{Q}, W_{K}, W_{1}, W_{2}, W_{U} = I_{2 \\times 2}$ and $\\sigma(x) = Relu(x)=max(0, x)$  and \n",
    "\\begin{align} W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} \\ \\text{and} \\ W_{P} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\end{align}\n",
    "  Given an input $x=[1]$, $n = n_{max} = 1$, $m = d = k = p = 2$ and $L = 1$, $\\alpha$ must be larger than 1 to get $\\hat{z} = [1]$ as a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&X = \\text{onehot}(x) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\\\\n",
    "&z_{0} = W_{E}X + [W_{P}]_{0:n} = \\begin{bmatrix}1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\n",
    "\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ \\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "&z_{1/2} = f_{d}^{A}(z_0) = z_0 + W_{O}^{T}W_{V}z_{0}A(z_0), \\ W_{O}^{T}W_{V} = I_{2 \\times 2}I_{2 \\times 2} = 1 \\\\\n",
    "&= z_0 + z_0 \\text{softmax}_{\\text{col}}(z^{T}W_{Q}^{T}W_{K}z + D), \\ D = [0], \\ W_{Q}^{T}W_{K} = I_{2 \\times 2}I_{2 \\times 2} = 1\\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} \\text{softmax}_{\\text{col}}(1 + \\alpha^2) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} \\\\\n",
    "\n",
    "&z_1 = f_1^{F}(z_{1/2}) = z_{1/2} + W_2^{T}\\sigma(W_1z_{1/2}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma\\left(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}\\right) \\\\ \n",
    "& = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix},\\ \\text{given} \\ \\alpha > 0 \\\\\n",
    "\n",
    "&Z = \\text{softmax}_{\\text{col}}\\left(\\begin{bmatrix}4 \\\\ \\alpha\\end{bmatrix}\\right) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{4\\alpha}} \\\\ \\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} \\end{bmatrix} \\\\\n",
    "&\\hat{z} = \\text{argmax}_{\\text{col}}(Z) \\\\\n",
    "\n",
    "&\\frac{e^{4\\alpha}}{e^4+e^{4\\alpha}} > \\frac{e^{4}}{e^4+e^{4\\alpha}}, \\ \\text{if} \\ \\hat{z} = [1] \\\\\n",
    "&e^{4\\alpha} > e^{4} \\\\\n",
    "&4\\alpha > 4 \\\\\n",
    "&\\alpha > 1  \\ \\blacksquare\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works given $\\alpha > 0$. For $\\alpha \\leq 0$ one must look from eq. (6) where $\\sigma = max(0, x)$\n",
    "\n",
    "\\begin{align}\n",
    "&z_1 = f_1^{F}(z_{1/2}) = z_{1/2} + W_2^{T}\\sigma(W_1z_{1/2}) = \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\sigma\\left(\\begin{bmatrix} 2 \\\\ 2\\alpha \\end{bmatrix}\\right) \\\\\n",
    "&= \\begin{bmatrix}2 \\\\ 2\\alpha \\end{bmatrix} + \\begin{bmatrix}2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix},\\ \\text{given} \\ \\alpha \\leq 0 \\\\\n",
    "\n",
    "&Z = \\text{softmax}_{\\text{col}}\\left(\\begin{bmatrix} 4 \\\\ 2\\alpha \\end{bmatrix}\\right) = \\begin{bmatrix} \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} \\end{bmatrix} \\\\\n",
    "&\\hat{z} = \\text{argmax}_{\\text{col}}\\left(\\begin{bmatrix} \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} \\end{bmatrix}\\right) \\\\\n",
    "\\text{if} \\ \\hat{z} = [1] \\ \\text{then} \\ &\\frac{e^{2\\alpha}}{e^4+e^{2\\alpha}} > \\frac{e^4}{e^4+e^{2\\alpha}} \\\\ \n",
    "&\\nexists \\ \\alpha \\leq 0 \\ \\text{s.t.} \\ e^{2\\alpha} > e^{4}\\\\\n",
    "&\\rightarrow \\alpha > 0\n",
    "\n",
    "\\end{align}\n",
    "and therefore $\\alpha > 1$ as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1\n",
    "\n",
    "To perform a step of gradient descent of a `NeuralNetwork`, the member function `step_gd` is called. This function then calls on the function `step_gd` in the class `Layer`. `Layer` is the base class for alot of the other classes used in this project, for example` Attention`, `SoftMax` and `LinearLayer`. These child classes will then inherit the member function `step_gd` from their parent class, `Layer`. The objects initialzed from these classes make up the neural network and then inherit the function `step_gd` from `Layer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep all the parameters (dimensions, length of input/output, number of iterations etc.) at one place we decided to store these in separate dataclasses in `train_test_params.py`. See the comments in `BaseParams` for an explanation of the member variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_params import *\n",
    "\n",
    "sort_params1 = SortParams1()    # parameters in part 1 of exercise 3.3\n",
    "sort_params2 = SortParams2()    # parameters in part 2 of exercise 3.3\n",
    "add_params = AddParams()\n",
    "text_params = TextGenParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "`test_implementation.ipynb` was used to test the implementation of the layers. We had to make a few changes to the tests:\n",
    "\n",
    "When computing the value of our loss function we have to slice Z such that we only use the columns that are useful.\n",
    "\n",
    "How to slice is determined by making sure that all $x$'s and $y$'s follow the dimensions given in part 3.2.1 in the project description.\n",
    "\n",
    "```python\n",
    "L = loss.forward(Z[:, :, -n_y:], y)\n",
    "```\n",
    "In addition we have to take into account that the backward pass of Softmax is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z} := g_l \\odot z_l - \\text{sum}_\\text{col}(g_l \\odot S) \\odot P.\n",
    "\\end{align}\n",
    "\n",
    "Here $g_l \\in \\R^{m \\times n_y}$ and $z_l \\in \\R ^{m \\times n}$. In order to match the dimensions we left-pad the gradient with zeros:\n",
    "\n",
    "```python\n",
    "pad_matrix = np.zeros((b, m, n_max - n_y))\n",
    "grad_Z = np.concatenate((pad_matrix, grad_Z), axis=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "The file `train_network.py` has two functions:\n",
    "\n",
    "- `init_neural_network`:  creates an object of type `NeuralNetwork` given the parameters that the neural network will use.\n",
    "\n",
    "- `train_network`: is the implementation of Algorithm 4 in the assignment. It trains the neural network and uses the Adam algorithm to change the individual paramteres of the network for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Sorting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple sorting of binary sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training and testing data for the sorting problem described in the first part of exercise 3.3.\n",
    "\n",
    "Here we sort sequences only consisting of zeros and ones. A quick example never hurts:\n",
    "\n",
    "$$\n",
    "[0,1,1,1,0] \\to [0,0,1,1,1]\n",
    "$$\n",
    "\n",
    "is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params1.r,\n",
    "    num_ints=sort_params1.m,\n",
    "    samples_per_batch=sort_params1.D,\n",
    "    n_batches_train=sort_params1.b_train,\n",
    "    n_batches_test=sort_params1.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params1.r - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(sort_params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using `CrossEntropy` as the loss function (object function).\n",
    "\n",
    "If the value of our object funcion $\\mathcal{L}$ gets lower than $0.01$ we stop the training.\n",
    "Otherwise the training runs for $n_{\\text{iter}} = 300$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params1.alpha,\n",
    "    n_iter=sort_params1.n_iter,\n",
    "    num_ints=sort_params1.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(L)\n",
    "ax.set(title=\"Object function\", ylabel=r\"$\\mathcal{L}$\", xlabel=\"iteration step\", yscale=\"log\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load a pre-trained network from a pickle dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = init_neural_network(sort_params1)\n",
    "network.load(\"filename\")            # this will override all parameters initialized by init_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the object function has converged to a minima. This is well supported by testing the network on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=sort_params1.m\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cranking up the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the sequence length $r = 5$ and vocabulary size $m = 2$. The total amount of different sequences that can be generated will is $2^5=32$. Since $2500$ sequences are created to train the model, there will most likely be extremely few or no new sequences to test the model on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the model to sort sequences where $r = 7$ and $m = 5$. This will result in $5^7 = 78125$ possible input sequences and thus we expect the testing data to contain unknown sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "training_data = get_train_test_sorting(\n",
    "    length=sort_params2.r,\n",
    "    num_ints=sort_params2.m,\n",
    "    samples_per_batch=sort_params2.D,\n",
    "    n_batches_train=sort_params2.b_train,\n",
    "    n_batches_test=sort_params2.b_test,\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, sort_params2.r - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(sort_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=sort_params2.alpha,\n",
    "    n_iter=sort_params2.n_iter,\n",
    "    num_ints=sort_params2.m,\n",
    "    dump_to_pickle_file=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(L)\n",
    "ax.set(title=\"Object function\", ylabel=r\"$\\mathcal{L}$\", xlabel=\"iteration step\", yscale=\"log\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=sort_params2.m\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: https://datascience.stackexchange.com/questions/25024/strange-behavior-with-adam-optimizer-when-training-for-too-long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4: Addition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of integer addition. Given two two-digit non-negative integers we want to train our model to predict the sum.\n",
    "Going through the same procedure as for the problem of sorting integers.\n",
    "\n",
    "Note that we now have $10^4$ possible input sequences: $a_0, a_1, b_0, b_1 \\in \\{0,1,2,\\dots,9\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "network = init_neural_network(add_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "\n",
    "# prepare training and test data for addition problem\n",
    "training_data = get_train_test_addition(\n",
    "    n_digit = add_params.r,\n",
    "    samples_per_batch = add_params.D,\n",
    "    n_batches_train = add_params.b_train,\n",
    "    n_batches_test=add_params.b_test\n",
    ")\n",
    "\n",
    "x_train = training_data[\"x_train\"]\n",
    "y_train = training_data[\"y_train\"][:, :, add_params.r*2 - 1:]\n",
    "x_test = training_data[\"x_test\"]\n",
    "y_test = training_data[\"y_test\"][:, :, ::-1]    # remember that (c0, c1, c2) is reversed in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "\n",
    "L = train_network(\n",
    "    network=network,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    loss_func=loss,\n",
    "    alpha=add_params.alpha,\n",
    "    n_iter=add_params.n_iter,\n",
    "    num_ints=add_params.m,\n",
    "    dump_to_pickle_file=True,\n",
    "    file_name_dump=\"nn_addition.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(L)\n",
    "ax.set(title=\"Object function\", ylabel=r\"$\\mathcal{L}$\", xlabel=\"iteration step\", yscale=\"log\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_network import test_trained_network\n",
    "\n",
    "test_trained_network(\n",
    "    network=network, x_test=x_test, y_test=y_test, num_ints=add_params.m\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_net = init_neural_network(add_params)\n",
    "load_net.load(\"nn_addition.pkl\")\n",
    "# train_network(load_net,x_train, y_train, loss, add_params.alpha, n_iter=1, num_ints=add_params.m)\n",
    "test_trained_network(load_net, x_test, y_test, num_ints=add_params.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konklusjon\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus task: Text generation\n",
    "\n",
    "The cells below are copied from the provided `text_generation`-notebook with a few additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import text_to_training_data\n",
    "\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    data,idx_to_text,text_to_idx, m = text_to_training_data(text_params.n_max,text,num_batches=text_params.b_train,batch_size=text_params.D)\n",
    "\n",
    "    print(\"We will train on %d batches of size %d\" % (len(data['x_train']),len(data['x_train'][0])))\n",
    "    print(\"Each sequence has length %d\" % text_params.n_max)\n",
    "\n",
    "    print(\"Example of a sequence (chars): \\n\")\n",
    "    print(''.join([idx_to_text[i] for i in data['x_train'][0][0]]))\n",
    "\n",
    "    print(\"\\nExample of a sequence (idx): \\n\")\n",
    "    print(data['x_train'][0][0])\n",
    "\n",
    "    text_params.m = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import init_neural_network\n",
    "\n",
    "net = init_neural_network(text_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network turns out to give (somewhat) decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_network\n",
    "from layers_numba import CrossEntropy\n",
    "\n",
    "loss = CrossEntropy()\n",
    "train_network.train_network(\n",
    "    network=net,\n",
    "    x_train=np.array(data[\"x_train\"]),\n",
    "    y_train=np.array(data[\"y_train\"]),\n",
    "    loss_func=loss,\n",
    "    alpha=text_params.alpha,\n",
    "    n_iter=text_params.n_iter,\n",
    "    num_ints=text_params.m,\n",
    "    dump_to_pickle_file=True,\n",
    "    file_name_dump=\"nn_text_gen.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "\n",
    "#We can now generate text from an initial string\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "#length of the total text sequence we want to generate\n",
    "n_gen = 10*text_params.n_max\n",
    "\n",
    "generated_idx = generate(net,start_idx,m,text_params.n_max,n_gen)\n",
    "\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison here we generate text *without* training the network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_generation import generate\n",
    "import numpy as np\n",
    "from train_network import init_neural_network\n",
    "\n",
    "net = init_neural_network(text_params)\n",
    "\n",
    "#We can now generate text from an initial string\n",
    "start_text = \"Thou shall not\"\n",
    "start_idx = np.array([text_to_idx[ch] for ch in start_text])[None]\n",
    "\n",
    "#length of the total text sequence we want to generate\n",
    "n_gen = 10*text_params.n_max\n",
    "\n",
    "generated_idx = generate(net,start_idx,m,text_params.n_max,n_gen)\n",
    "\n",
    "text = ''.join([idx_to_text[idx] for idx in generated_idx[0]])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And then the animator suffered a fatal heart attack \n",
    "\n",
    "[Monty Python](https://www.youtube.com/watch?v=3Q2WPneqhhs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitber_indmatprosjekt_transformermodell-j3GqZX7I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
